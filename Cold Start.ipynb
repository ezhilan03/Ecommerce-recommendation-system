{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a78d91",
   "metadata": {},
   "source": [
    "# Part 1:\n",
    "\n",
    "First we're gonna do sentiment analysis on the twitter data to categorise the tweets into positive, neutral and negative tweets.\n",
    "We choose VADER (Valence Aware Dictionary and sEntiment Reasoner) because it is a rule-based sentiment analysis tool specifically designed for social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1538998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Twitter_Data.csv\")\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['clean_text', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee342a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6235b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df1a43f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b977d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions (e.g., @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ca7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf22001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Data/preprocessedtweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"Data/preprocessedtweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a2e50",
   "metadata": {},
   "source": [
    "## Sentiment Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SentimentIntensityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Function to assign sentiment\n",
    "def assign_sentiment(tweet):\n",
    "    sentiment_scores = analyzer.polarity_scores(tweet)\n",
    "    if sentiment_scores['compound'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262aae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis on training data\n",
    "pred_sentiment = df['clean_text'].apply(assign_sentiment)\n",
    "#temp_sentiment = df['Sentiment'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(df['category'], pred_sentiment))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(df['category'], pred_sentiment)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68b9cf",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5773ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df_temp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Function to encode text and labels for training\n",
    "def encode_data(texts, labels=None):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    if labels is not None:\n",
    "        labels = torch.tensor(labels)  # Ensure labels are tensor\n",
    "        return inputs, labels\n",
    "    return inputs\n",
    "\n",
    "# Assume train_df and test_df are already defined DataFrames\n",
    "train_inputs, train_labels = encode_data(train_df['clean_text'].tolist(), train_df['category'].tolist())\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in tqdm(range(0, len(train_inputs['input_ids']), batch_size)):\n",
    "        batch_inputs = {key: val[i:i+batch_size] for key, val in train_inputs.items()}\n",
    "        batch_labels = train_labels[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch_inputs, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss}\")\n",
    "\n",
    "# Encode test data\n",
    "test_inputs, test_labels = encode_data(test_df['clean_text'].tolist(), test_df['category'].tolist())\n",
    "\n",
    "# Evaluate the model using the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(classification_report(test_labels, predicted_labels))\n",
    "\n",
    "# Optionally save the test results to a new CSV file\n",
    "test_df['predicted_sentiment'] = predicted_labels.numpy()\n",
    "test_df.to_csv('test_sentiment_analysis_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a24740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764db57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5779c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6de559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score(train_df['clean_text'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79115d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'].iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1d76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis on a batch\n",
    "def batch_sentiment_score(reviews):\n",
    "    tokens = tokenizer(reviews, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    return torch.argmax(outputs.logits, dim=1).cpu().numpy() + 1\n",
    "\n",
    "# Apply sentiment analysis in batches\n",
    "batch_size = 32\n",
    "sentiments = []\n",
    "model.train()\n",
    "for i in tqdm(range(0, len(train_df), batch_size)):\n",
    "    batch_reviews = train_df['clean_text'][i:i+batch_size].tolist()\n",
    "    batch_sentiments = batch_sentiment_score(batch_reviews)\n",
    "    sentiments.extend(batch_sentiments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('sentiment_analysis_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb76fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['sentiment'] = train_df['sentiment'].apply(lambda x: 1 if x > 3 else -1 if x < 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de773bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis in batches for the test set\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_sentiments = []\n",
    "for i in tqdm(range(0, len(test_df), batch_size)):\n",
    "    batch_reviews = test_df['clean_text'][i:i+batch_size].tolist()\n",
    "    batch_sentiments = batch_sentiment_score(batch_reviews)\n",
    "    test_sentiments.extend(batch_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test set\n",
    "y_true = test_df['category'].astype(int)\n",
    "y_pred = test_sentiments\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d4fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944650b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with sentiment values 0 and 1. \n",
    "tweet_final = df[df['category'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191bf90",
   "metadata": {},
   "source": [
    "Train the model to find categories with product description data. Use the model to find categories of tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea81fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# List all CSV files in a directory\n",
    "csv_files = glob.glob('Data/Product_ratings/*.csv')  # Update the path as per your file location\n",
    "\n",
    "# Initialize an empty list to store individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file and read it into a DataFrame, then append it to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)  # Set ignore_index=True to reindex rows\n",
    "\n",
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnamed column\n",
    "combined_df = combined_df.drop(combined_df.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65957e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = combined_df['main_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b33f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "text_vectors = vectorizer.fit_transform(tweet_final['tweet'])\n",
    "category_vectors = vectorizer.transform(categories)\n",
    "\n",
    "# Similarity measurement\n",
    "similarity_matrix = cosine_similarity(text_vectors, category_vectors)\n",
    "\n",
    "# Assign categories\n",
    "threshold = 0.2  # Define a threshold for similarity\n",
    "assigned_categories = []\n",
    "\n",
    "for idx, similarities in enumerate(similarity_matrix):\n",
    "    max_similarity = max(similarities)\n",
    "    if max_similarity >= threshold:\n",
    "        assigned_category = categories[similarities.argmax()]\n",
    "    else:\n",
    "        assigned_category = None\n",
    "    assigned_categories.append(assigned_category)\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results_df = pd.DataFrame({'Text': tweet_final['tweet'], 'Assigned Category': assigned_categories})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.dropna(subset=['Assigned Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['Assigned Category'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
